---
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    number_sections: true
    toc: true
    toc_depth: 4
title: "Diabetes Risk prediction via SEMMA w/ Regularized Logistic Regression"
author: 
- Kevin Acosta^[kmacosta2@miners.utep.edu]
- M.S. in Data & Information Sciences (UTEP)
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontsize: 11pt
spacing: single
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb, bm}
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \rhead{}
- \lhead{}
- \cfoot{\thepage}
- \usepackage{algorithm}
- \usepackage[noend]{algpseudocode}

---
<!-- QUESTION ONE: WHAT --> 
\noindent\rule{17.5cm}{0.8pt}

# Data Import
```{r}
data <- read.csv("/Users/kevinacosta/Desktop/Stats ML 2/Comp Proj 01/diabetes_data_upload.csv")
dim(data)
summary(data)
head(data)[c(1,2,3),]
```

\section{Exploratory Data Analysis}

```{r}
table(data$class) # simple frequency table

for (i in 1:(ncol(data)-1)) {
  cat("Column:", colnames(data)[i],"--> ")
  x <- data[,i]
  cat("Number of unique values:", length(unique(x, incomparables=TRUE)),"\n")
  # checking for missing values per column
  col_count <- sum(is.na(x))
  
  if(col_count>0) {
    cat("Missing Values:", col_count, "\n")
  }
  else {
    cat("\t\tNONE Missing for this column\n")
  }
  
}

```
After seeing the first frequency table, we can confirm this does resemble an unbalanced classification problem favoring 'positive' labels with 200 negative cases for Diabetes and 320 positive cases.
Using a for loop I checked for every column/feature excluding our response variable and confirmed that there are no missing values found.

\section{Variable Screening}

```{r}
#library(tidyverse)
response <- data$class
p_values <- numeric(ncol(data) - 1)

data2 <- data
data2[, 1:(ncol(data2) - 1)] <- lapply(data2[, 1:(ncol(data2) - 1)], as.factor)

for (i in 1:(ncol(data2) - 1)) {
  attrib <- data2[, i]
  
  if (is.factor(attrib)){ # categorical attribute --> chi-squared test
    cat_attr <- chisq.test(table(attrib, response))
    p_values[i] <-  cat_attr$p.value
  }
  else{ # continuous attribute --> t-test
    cont_attr <- t.test(attrib ~ response)
    p_values[i] <- cont_attr$p.value
  }
  cat("Attribute: ", colnames(data2)[i], "\n\t\tP-value: ", p_values[i], "\n")
}
# identifying the statistically significant attributes
significant_attrs <- which(p_values < 0.25 & !is.na(p_values))
significant_names <- colnames(data2)[significant_attrs]

print("Significant attributes based on level alpha 0.25:\n")
print(significant_names)
```

\section{Data Partition}

Partition the data into two parts, the training data D1 and the test data
D2, with a ratio of 2:1.
```{r}
data$Itching <- NULL
data$delayed.healing <- NULL

n_samples <- nrow(data)
# 2/3 for train set below
n_train <- round(n_samples * 0.67) - 1
train <- sample(n_samples, n_train)
train_data <- data[train,]
test_data <- data[-train,]

dim(train_data)
dim(test_data)
```


\section{Logistic Regression Modeling}


```{r}
library(glmnet)

y <- as.factor(train_data$class)
X <- model.matrix(class ~ ., data=train_data)[,-1] #
# fitting a lasso logistic regression with cv to help select the best lambda value.
cv_lasso_fit <- cv.glmnet(x=X,y=y, nfolds=10, family="binomial", alpha=1)
best_lambda <- cv_lasso_fit$lambda.min
#plot(cv_lasso_fit)

# here we're simply fitting again but to include the best lambda & afterwards 
# inspect the coefficients
best_lasso <- glmnet(x=X, y=y, family="binomial", alpha=1, lambda = best_lambda)
cat("Best lambda:", best_lambda)

print("Final lasso model coefficients:")
print(coef(best_lasso))

```
The model coefficient results presented above can vary from run to run, but as long as they have a value not zero, we would list them as significant to the model.

In general and on average, the most relevant coefficients are: 
'PolydipsiaYes'- (a lot of thirst is indicated with the highest positive coefficient), 
'PolyuriaYes'- (excessive urination is also indicated by a large coefficient, so its another strong predictor of diabetes)
'GenderMale'- (so just being male significantly decreases the odds of getting a diabetes diagnosis because the coefficient is negative!).

\section{Model Assessment/Deployment}

```{r}
library(pROC)
y_test <- as.factor(test_data$class)
x_test <- model.matrix(class ~ ., data=test_data)[,-1]

# predict probability for a positive classification/diagnosis
y_pred <- predict(best_lasso, newx=x_test, type="response")

# generate ROC curve and get AUC
roc_curve <- roc(y_test, y_pred)
auc_value <- auc(roc_curve)

plot(roc_curve, col = "blue", main = "ROC Curve: Logistic Regression w/LASSO", print.auc = TRUE)

print(paste("AUC/C-Statistic:", round(auc_value, 4)))
```
Overall the Logistic Regression Classifier's performance is very good at predicting what is considered the true positives in classifying patients with diabetes when they do in fact have it.

